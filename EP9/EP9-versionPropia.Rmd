---
title: "Ejercicio 9 version mia"
author: "Grupo  3"
date: "2023-12-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#############
# Librerías #
#############

library(dplyr)
library(tidyverse)
library(ggpubr)
library(caret)
library(car)

```


```{r}
set.seed(2122)
datos <- read.csv2("EP09 Datos.csv", sep = ";")

# muestra 50 mujeres 
mujeres <- datos[datos$Gender == 0, ]
muestra <- mujeres[sample(1:nrow(mujeres),size = 50), ]

# selección 8 predictores
predictores <- names(datos)
predictores_seleccionados <- sample(predictores, 8)

# seleccion predictor del resto en funcion de la correlacion
predictores_resto <- setdiff(predictores,predictores_seleccionados)
getCor <- function(predictores, data){
  for (i in predictores) {
    cor_test <- cor.test(data$Weight, data[[i]], method = "pearson")
    cat("Coeficiente de correlación para", i, ":", cor_test$estimate, "\n")
  }
}
getCor(predictores_resto, muestra) # se selecciona la de mayor correlacion -> Hip.Girth

# Se crea el modelo con el predictor escogido
# este modelo tiene un R^2 = 0.8135, lo que significa que explica aproximadamente el 81% de la variabilidad de la respuesta.
# aun existe un 19% que falta por explicar.
modelo <- lm(mujeres$Weight ~ mujeres$Hip.Girth, data = mujeres)
summary(modelo)


# Buscar predictores entre los 8 para predecir. 

# Creamos un modelo completo con los 8 predictores antes seleccionados

predictores_seleccionados_datos <- select(muestra, one_of(predictores_seleccionados))
predictores_seleccionados_datos$Weight <- muestra$Weight
predictores_seleccionados_datos$Hip.Girth <- muestra$Hip.Girth

# Se construye el modelo nulo, puesto que se va a comparar, este no deberia tener ningún predictor pero se iniciará con 1 por el enunciado del ejercicio.
modelo_nulo_ideal <- lm(muestra$Weight ~ 1, data = predictores_seleccionados_datos)
modelo_nulo <- lm(muestra$Weight ~ 1, data = predictores_seleccionados_datos) # aunque crei que se debia usar el que tenia 1, se usa el nulo, si se hiciera a mano ahi si se puede usar el que tiene solo 1
summary(modelo_nulo)

# Modelo completo, espera el maximo de predictores.
modelo_completo <- lm(muestra$Weight ~ ., data = predictores_seleccionados_datos)
summary(modelo_completo)

# A mano se puede hacer el add1, drop1 y update; pero tambien se puede con: 
# 1. Seleccion hacia adelante
adelante <- step(modelo_nulo, scope = list(upper = modelo_completo), direction = "forward", trace = 0)
summary(adelante)
cat("AIC adelante: ", AIC(adelante))

# 2. Seleccion hacia atras
atras <- step(modelo_completo, scope = list(lower = modelo_nulo), direction = "backward", trace = 0)
summary(atras)
cat("AIC atras: ", AIC(atras))

# 3. escalonado 
escalonado <- step(modelo_nulo, scope = list(lower = modelo_nulo, upper = modelo_completo), direction = "both", trace = 0)
summary(escalonado)
cat("AIC escalonado: ", AIC(escalonado))


# Se avalua el modelo -> en este caso: adelante
# Para evaluar tenemos 4 cosas principales

# 1. Valores atipicos
adelante$coefficients
predictores_modelo_adelante <- names(coef(adelante))[-1]
datos_p <- muestra[, c(predictores_modelo_adelante, "Weight")]
datos_p

resultados <- data.frame(respuesta_predicha = fitted(adelante))


resultados[["residuos_estandarizados"]] <- rstandard(adelante)
resultados[["residuos_estudiantizados"]] <- rstudent(adelante)
resultados[["distancia_Cook"]] <- cooks.distance(adelante)
resultados[["dfBeta"]] <- dfbeta(adelante)
resultados[["dffit"]] <- dffits(adelante)
resultados[["apalancamiento"]] <- hatvalues(adelante)
resultados[["covratio"]] <- covratio(adelante)

cat("Identificación de valores atípicos: \n")

# Buscamos las observciones con residuos estandarizados fuera del 95% esperado
sospechoso1 <- which(abs(resultados[["residuos_estandarizados"]]) > 1.96)
cat("- Residuos estandarizados fuera del 95% esperado: ", sospechoso1, "\n")

# Buscamos observaciones con distancia de Cook mayor a 1
sospechoso2 <- which(resultados[["distancia_cook"]] > 1)
cat("- Residuos con una distancia de Cook alta:", sospechoso2, "\n")

# Buscamos observaciones con apalancamiento mayor igual al doble del apalacamiento promedio
apal_medio <- (ncol(datos_p) + 1) / nrow(datos_p)
sospechoso3 <- which(resultados[["apalancamiento"]] > 2 * apal_medio)
cat("- Residuos con apalancamiento fuera del rango: ", sospechoso3, "\n")

sospechoso4 <- which(apply(resultados[["dfBeta"]] >= 1, 1, any))
names(sospechoso4) <- NULL
cat("- Residuos con DFBeta >= 1:", sospechoso4, "\n")

# Buscamos observaciones con razón de covarianza fuera de rango
inferior <- 1 - 3 * apal_medio
superior <- 1 + 3 * apal_medio
sospechoso5 <- which(resultados[["covratio"]] < inferior | resultados[["covratio"]] > superior)
cat("- Residuos con razón de covarianza fuera de rango: ", sospechoso5, "\n")

# Resumen de valores sospechosos

sospechosos <- c(sospechoso1, sospechoso2, sospechoso3, sospechoso4, sospechoso5)

sospechosos <- sort(unique(sospechosos))

cat("\nResumen de valores sospechosos: \n")
cat("Apalancamiento promedio: ", apal_medio, "\n")
cat("Intervalo razón de covarianza: [", inferior, "; ", superior, "]\n\n", sep = "")

print(round(resultados[sospechosos, c("distancia_Cook", "apalancamiento", "covratio")], 3))

# 2. Verificacion de condiciones a modo que el modelo sea generalizable
# 2.1 Independencia de los residuos
cat("Prueba de Durbin-Watson para autocorrelaciones ")
cat("entre errores:\n")
print(durbinWatsonTest(adelante)) # p-value > 0.05 concluye que son independientes, tener cuidado con el orden de los datos

# 2.2 Distribucion normal de los residuos
cat("Prueba de noramalidad para los residuos:\n")
print(shapiro.test(adelante$residuals)) # p-value > 0.05, siguen una distribucion normal

# 2.3 Homocedasticidad de los residuos
cat("Prueba de homocedasticidad para los residuos:\n")
print(ncvTest(adelante))

# 2.4 Multicolinealidad
vifs <- vif(adelante)
cat("Verificar la multicolinealidad:\n")
cat("- VIFs: \n")
print(vifs)
cat("- Tolerancias:\n")
print(1/vifs)
cat("- VIF medio: ", mean(vifs), "\n")

# Considerar los siguientes para el VIF: 
# Algunos autores usan el valor VIF >= 10 como problematico, otrs 5 y otros 2,5
# Se considera un VIF prom >= 1 podria generar sesgo en el modelo
# Sobre la tolerancia: 
# valores bajo 0,2 podrian ser problematicos, algunos creen que valores cercanos a 0,4 deberían ser revisados


# 3. Validación cruzada -> Buscamos verificar si el modelo puede generalizarse
#    Se busca evaluar como se comporta el modelo con dato que no ha visto (prueba - entrenamiento) -> metrica: MSE
# Los datos son los mismos para las 3 formas 
n = nrow(predictores_seleccionados_datos)
n_entrenamiento <- floor(.7 * n)
muestra_vc <- sample.int(n = n, size = n_entrenamiento, replace = FALSE)
entrenamiento <- predictores_seleccionados_datos[muestra_vc, ]
prueba <- predictores_seleccionados_datos[-muestra_vc, ]

# Tenemos varias formas:
# 3.1 VC normal?

# Se ajusta un modelo con los datos de entrenamiento (el mismo de antes pero otros datos)
modelo_entrenamiento <- lm(Weight ~ Hip.Girth + Chest.diameter + Elbows.diameter + Bicep.Girth + Navel.Girth, data = entrenamiento)
print(summary(modelo_entrenamiento))

# Se calcula el error cuadrático promedio para el conjunto de entrenamiento
mse_entrenamiento <- mean(modelo_entrenamiento$residuals ** 2)
cat("MSE entrenamiento: ", mse_entrenamiento, "\n")

# Predicciones con el conjunto de prueba
predicciones_prueba <- predict(modelo_entrenamiento, prueba)

# Se calcula el error cuadratico promedio para el conjunto de entrenamiento
error <- prueba[["Weight"]] - predicciones_prueba
mse_prueba <- mean(error ** 2)
cat("MSE prueba: ", mse_prueba, "\n")

# Este modelo entrega valores similares, por lo que el modelo podría ser generalizable
# Si la diferencia entre los MSE es muy grande, se sugiere que el modelo está sobre ajustado, lo que implicaría que el modelo se ajusta bien a los datos de entrenamiento pero no a los de prueba. Lo cual haría imprudente implicar que es generalizable. Aunque esto también se puede deber a la separación aleatoria de los datos.

# 3.2 VC de k pliegues

# Se ajusta el modelo con 5 pliegues
modelo_entrenamiento2 <- train(Weight ~ Hip.Girth + Chest.diameter + Elbows.diameter + Bicep.Girth + Navel.Girth, data = entrenamiento, method = "lm", trControl = trainControl(method = "cv", number = 5))
print(summary(modelo_entrenamiento2))

# predicciones para el conjunto de entrenamiento
predicciones_entrenmiento2 <- predict(modelo_entrenamiento2, entrenamiento)

# Se calcula el error cuadrático promedio para el conjunto de entrenamiento
error_entrenamiento2 <- entrenamiento[["Weight"]] - predicciones_entrenmiento2
mse_entrenamiento2 <- mean(error_entrenamiento2 ** 2)
cat("MSE entrenamiento: ", mse_entrenamiento2, "\n")

# Predicciones con el conjunto de prueba
predicciones_prueba2 <- predict(modelo_entrenamiento2, prueba)

# Se calcula el error cuadratico promedio para el conjunto de entrenamiento
error <- prueba[["Weight"]] - predicciones_prueba2
mse_prueba2 <- mean(error ** 2)
cat("MSE prueba: ", mse_prueba2, "\n")


# 3.3 VC dejando uno fuera
# Se ajusta el modelo con el parametro "LOOCV"
modelo_entrenamiento3 <- train(Weight ~ Hip.Girth + Chest.diameter + Elbows.diameter + Bicep.Girth + Navel.Girth, data = entrenamiento, method = "lm", trControl = trainControl(method = "LOOCV", number = 5))
print(summary(modelo_entrenamiento3))

# predicciones para el conjunto de entrenamiento
predicciones_entrenmiento3 <- predict(modelo_entrenamiento3, entrenamiento)

# Se calcula el error cuadrático promedio para el conjunto de entrenamiento
error_entrenamiento3 <- entrenamiento[["Weight"]] - predicciones_entrenmiento3
mse_entrenamiento3 <- mean(error_entrenamiento3 ** 2)
cat("MSE entrenamiento: ", mse_entrenamiento3, "\n")

# Predicciones con el conjunto de prueba
predicciones_prueba3 <- predict(modelo_entrenamiento3, prueba)

# Se calcula el error cuadratico promedio para el conjunto de entrenamiento
error <- prueba[["Weight"]] - predicciones_prueba#
mse_prueba3 <- mean(error ** 2)
cat("MSE prueba: ", mse_prueba3, "\n")

# 4 Tamaño de la muestra

# Mientras mas mejor, pero una de las reglas mas simplistas es verificas que se tengan al menos 10 o 15 observaciones por cada predictor.





```


















